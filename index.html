<!--  -->


<!-- --------------------------------------------------------- -->
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="segmentation in the loop for recognition, hierarchical segmentation, part-to-whole recognition, vision transformer">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="https://CAST.github.io//static/images/preview_new.jpeg" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="628" />
  
  <article class="post-content">
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="CAST" />
    <meta name="twitter:description"
        content="Learning Hierarchical Image Segmentation For Recognition and By Recognition" />
    <meta name="twitter:url" content="https://CAST.github.io/" />
    <meta name="twitter:image" content="https://CAST.github.io/static/images/preview_new.jpeg" />
    <meta name="twitter:image" content="https://CAST.github.io/static/images/preview_new.jpeg" />
    <meta name="twitter:image:src" content="https://CAST.github.io/static/images/preview_new.jpeg" />
    <meta name="twitter:image_alt" content="CAST" />
  <article class="post-content">

  <title>Learning Hierarchical Image Segmentation For Recognition and By Recognition</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <link rel="stylesheet" href="https://fonts.sandbox.google.com/css2?family=Material+Symbols+Rounded:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200" />


 <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  

  <style>
    .material-symbols-rounded {
        font-variation-settings: 'FILL' 1,
        'wght' 400,
        'GRAD' 0,
        'opsz' 48
    }


    .linkscontainer {
        max-width: 600px;
        margin: 0 auto;
    }

    .links {
        /*font-size: 5rem;*/
        /*margin: 0 -20px;*/
        display: flex;
        gap: 20px;
        justify-content: center;
        flex-wrap: wrap;
    }

    .links a span.material-symbols-rounded {
        max-width: 25px;
    }

    span.material-symbols-rounded {
        max-width: 25px;
    }

    .links a {
        display: inline-flex;
        /*flex-direction: column;*/
        align-items: center;
        text-decoration: none;
        border-radius: 5px;
        padding: 5px 7px;
        color: #f6f3f1 !important;
        font-family: 'GT Ultra', sans-serif !important;
        font-weight: 400;
        /*margin: 0 10px;*/
        transition: transform .2s;
        background-color: rgba(var(--btn-bgc), 1);
    }


    .links a:nth-child(1) {
        --btn-bgc: 237, 100, 90;
    }

    .links a:hover {
        transform: scale(1.2);
        color: #f6f3f1 !important;
    }

    .links a:active {
        transform: scale(1.3);
    }

    .links a:nth-child(2) {
        --btn-bgc: 47, 138, 196;
    }

    .links a:nth-child(3) {
        --btn-bgc: 229, 134, 6;
    }

    .links a:nth-child(4) {
        --btn-bgc: 133, 180, 51;
    }

    .links a:nth-child(5) {
        --btn-bgc: 204, 97, 176;
    }

    .links a span.material-symbols-rounded {
        margin-right: .25rem;
    }

    .links a:not(:last-child) {
        /*margin-right: 20px;*/
    }


    .links .material-symbols-rounded {
        font-size: 1.25rem;
    }


    .links i {
        font-size: 28px;
    }

    .fig {
        width: 100%;
        display: block;
        padding: 0 10px;
        margin: 0 auto;
    }

    img.arch {
        max-width: 500px;
    }

    img.blobs {
        max-width: 350px;
    }

    .teaser {
        text-align: center;
        margin-bottom: 1rem;
    }

    .teaser + section {
        margin-top: 1rem;
    }

    .teaser video {
        max-width: 600px;
        width: 100%;
    }

    img.teaser {
        max-width: 90%;
    }

    .abstract-imgs .col {
        display: flex;
        align-items: center;
    }

    .videowrapper {
        float: none;
        clear: both;
        width: 100%;
        position: relative;
        padding-bottom: 56.25%;
        padding-top: 25px;
        height: 0;
    }

    .wrapwrap {
        max-width: 800px;
        margin: 0 auto;
    }

    .videowrapper iframe {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
    }

    .abstract-capt {
        font-size: 0.8rem;
        display: block !important;
        text-align: center;
    }

    .latex {
        display: inline;
        font-family: 'Math', monospace;
        font-style: italic;
    }

    p {
        margin: 0 !important;
    }

    .bs-tooltip-end {
        margin-left: 3px !important;
    }

    section {
        margin: 2rem 0;
    }

    section h5 {
        margin: 1.5rem 0 0.75rem 0;
    }

    .iconbutton {
        padding: 0.1rem 0.3rem;
        display: flex;
        border: none !important;
        background-color: #4B495B !important;
        color: #f5ece5 !important;
        font-family: 'GT Ultra', sans-serif !important;
        font-weight: 400;
        text-decoration: none !important;
    }

    .iconbutton span.material-symbols-rounded {
        font-size: 1.5rem;
        /* font-weight: 700; */
    }

    .playpause span.material-symbols-rounded {
        font-variation-settings: 'wght' 700;


    }

    .iconbutton span.material-symbols-rounded:hover {
        color: #faf6f2 !important;
    }

    .iconbutton:hover, .iconbutton:focus {
        background-color: #2E2E38 !important;
        border-color: none !important;
        color: #faf6f2 !important;
    }

    .iconbutton:focus {

    }

    .iconbutton:active {
        background-color: #09090B !important;

    }

    [data-clipboard-target] {
        cursor: pointer;
    }

    [data-clipboard-target]:hover {
        color: #276FBF !important;
    }

    /* .emptyrooms {
         max-width: 80%;
     }*/

    .emptyrooms {
        display: flex;
        /*justify-content: center;*/
        align-items: flex-start;
        background-color: #f5ece5;
    }

    .playpause {
        flex-shrink: 0;
    }

    .emptyrooms img:first-child {
        width: 16.196944%;
        margin-right: 0.6%;
    }

    .emptyrooms img:last-child {
        width: 82.8862479%;
    }


    section h3 {
        margin-bottom: 1rem;
        display: flex;
        align-items: center;
    }

    section h5, section h3 {
        justify-content: space-between;

        display: flex;
        align-items: center;
    }

    h3, h5 {
        scroll-margin-top: 25px;
        /*display: inline-block;*/
    }

    h3[data-exclude-link], h5[data-exclude-link] {
        cursor: initial;
    }

    h3 span, h5 span {
        display: inline-flex;
        align-items: center;
        cursor: pointer;
    }

    h3 span:hover, h5 span:hover {

        color: #276FBF !important;
    }

    /*
            h3[data-exclude-link]:hover, h5[data-exclude-link]:hover {
                color: initial !important;
            }*/

    .carousel {
        margin: 1rem 0;
    }

    .ltx {
        vertical-align: baseline;
    }

    .cit {
        background-color: rgba(26, 25, 31, 0.05);
        padding: 10px;
        border-radius: 5px;
        font-size: 14px;
        display: inline-block;
        margin: 0 auto;
        overflow-y: hidden;
        overflow-x: auto;
        white-space: pre-wrap;
        white-space: -moz-pre-wrap;
        white-space: -pre-wrap;
        white-space: -o-pre-wrap;
        word-wrap: break-word;
        font-family: 'Code', monospace;
    }

    .cit_cont {
        display: flex;
    }

    .video-container {
        position: relative;
    }

    /* .video-container .video-border {
         position: absolute;
         width: 100%;
         height: 100%;
         top: 0;
         left: 0;
         box-shadow: inset 0px 0px 0px 6px #f5ece5;
     }*/

    .video-container video {
        width: 100%;
        display: block;
        clip-path: inset(5px 5px);
    }

    .video-container img {
        width: 100%;
    }

    .splide > * {
        font-weight: 500;
        position: initial;
    }

    .splide__arrow {
        position: initial;
        background: none;
        /*opacity: 1;*/
        -webkit-transform: none;
        -moz-transform: none;
        -ms-transform: none;
        -o-transform: none;
        transform: none;
        font-size: 1.25rem;
        justify-content: end;
        width: 1.5em;
    }

    .splide__arrow--prev {
        justify-content: left;
    }

    .splide__slide {
        height: 0;
    }

    .splide__slide.is-visible {
        height: auto;
    }

    .move-cont {
        display: flex;
    }

    .move-cont > video {
        max-width: 100%;
    }

    .splide__pagination__page.is-active {
        background: #000;
    }

    .splide__pagination__page {
        -webkit-transition: all 0.3s;
        -moz-transition: all 0.3s;
        -ms-transition: all 0.3s;
        -o-transition: all 0.3s;
        transition: all 0.3s;
    }

    .splide__pagination {
        margin-top: 0.25rem;
    }

    .splide__pagination__page:hover {
        background: #aaa;
        transform: scale(1.2);
    }

    .splide__track_and_arrows {
        display: flex;
    }

    .splide__arrows {
        display: flex;
        align-items: center;
    }

    .styletransfer {
        display: grid;
        grid-row-gap: .5rem;
        grid-template-columns: 1fr 20px 1fr 1fr 1fr 20px 1fr 1fr 1fr;
    }

    .inversion {
        display: grid;
        /*grid-row-gap: .5rem;*/
        /*grid-template-columns: 1fr 20px 1fr 1fr 1fr 20px 1fr 1fr 1fr;*/
        grid-template-columns: 1fr 1fr 1fr 1fr 1fr;
    }

    .styletransfer img {
        max-width: 100%;
        clip-path: inset(2px 2px);
    }

    .inversion span {
        text-align: center;
    }

    .inversion img {
        max-width: 100%;
        clip-path: inset(2px 2px);
    }


    #inversion ~ .splide img {
        max-width: 100%;
        padding: 0.3rem;
    }

    .styletransfer span {
        text-align: center;
    }

    .styletransfer span:nth-child(2) {
        grid-column-start: 3;
        grid-column-end: 6;
    }

    .styletransfer span:nth-child(3) {
        grid-column-start: 7;
        grid-column-end: 10;
    }


    @media (min-width: 992px) {
        .container {
            max-width: 1050px;
        }
    }

    .inversion_subheader {
        font-weight: bold;
        text-align: center;
        margin-top: 0.5rem;
    }

    .inversion_subheader:not(:first-of-type) {
        margin-top: 0.2rem;
    }

    .inversion_subheader + .splide {
        margin: 0.2rem 0 !important;
    }

    /*
            .move-cont .video-container:nth-child(8), .move-cont .video-container:nth-child(7) {
                display: none;
            }*/

    @media (max-width: 991px) {
        /*
                    .move-cont .video-container:nth-child(5), .move-cont .video-container:nth-child(6) {
                        display: none;
                    }*/
        .move-cont > video {
            max-width: 150%;
            clip-path: inset(0 33.333333% 0 0);
        }

        header h3 {
            font-size: 1.5rem;
        }

        .cit {
            /*font-size: 12px !important;*/
        }

        .emptyrooms img:last-child {
            width: 99.6% !important;
        }

        .emptyrooms img:first-child {
            width: 19.46% !important;
            margin-right: 0.7522% !important;
        }

        .styletransfer {
            display: grid;
            grid-template-columns: 1fr 15px 1fr 1fr 15px 1fr 1fr !important;
        }

        .styletransfer img:nth-child(9n+3), .styletransfer img:nth-child(9n-1) {
            display: none;
        }

        .styletransfer span:nth-child(2) {
            grid-column-start: 3;
            grid-column-end: 5;
        }

        .styletransfer span:nth-child(3) {
            grid-column-start: 6;
            grid-column-end: 8;
        }
    }

    .author {
        padding: 0 0.6rem;
    }

    @media (max-width: 767px) {
        html, body {

            /*font-size: 16px;*/
        }

        .author {
            font-size: 18px;
        }

        .insts {
            font-size: 16px;
        }

        /*.move-cont .video-container:nth-child(4) {
            display: none;
        }*/
        .move-cont > video {
            max-width: 200%;
            clip-path: inset(0 50% 0 0);
        }

        .inversion *:nth-child(5n+2) {
            display: none;
        }

        .inversion {
            grid-template-columns: 1fr 1fr 1fr 1fr;
        }

        .emptyrooms img:last-child {
            width: 124.477307% !important;
        }

        .emptyrooms img:first-child {
            width: 24.3243243% !important;
            margin-right: 0.9% !important;
        }

        .styletransfer {
            display: grid;
            grid-template-columns: 1fr 10px 1fr 10px 1fr !important;
        }

        .styletransfer img:nth-child(9n+2), .styletransfer img:nth-child(9n-2) {
            display: none;
        }

        .styletransfer span:nth-child(2) {
            grid-column-start: 3;
            grid-column-end: 4;
        }

        .styletransfer span:nth-child(3) {
            grid-column-start: 5;
            grid-column-end: 6;
        }
    }

    @media (max-width: 540px) {
        .links a span.material-symbols-rounded {
            max-width: 20px;
        }

        .links {
            gap: 12px;
        }
    }

    @media (max-width: 510px) {
        /*
                    .move-cont .video-container:nth-child(3) {
                        display: none;
                    }*/
        .move-cont > video {
            max-width: 300%;
            clip-path: inset(0 66.6666666% 0 0);
        }


        .inversion {
            grid-template-columns: 1fr 1fr;
        }

        .inversion span:nth-child(n+4) {
            grid-row: 3;
        }

        .links .material-symbols-rounded {
            /*display: none;*/
            font-size: 1rem;
        }

        .links a span.material-symbols-rounded {
            max-width: 18px;
        }

    }

    @media (max-width: 399px) {

    }

    @media (max-width: 380px) {
        /*.links .material-symbols-rounded {*/
        /*    display: none;*/
        /*    padding: 5px 10px;*/
        /*    !*font-size: 1rem;*!*/
        /*}*/
    }

    @media (max-width: 575px) {

        img.blobs {
            max-width: 300px;
        }

        .emptyrooms img:last-child {
            width: 165.941536% !important;
        }

        .emptyrooms img:first-child {
            width: 32.4269205% !important;
            margin-right: 1.15% !important;
        }
    }

    @media (min-width: 768px) {
        .abstract-imgs .col-md-7 {
            border-right: 1px solid #1f1e1d;
        }

        img.arch, img.blobs {
            max-width: 600px;
        }

    }


</style>
</head>
<!-- --------------------------------------------------------- -->
<body>

    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">Learning Hierarchical Image Segmentation For Recognition and By Recognition</h1>
              <div class="is-size-4 publication-authors">
                <span class="author-block">
                    <a href="http://twke18.github.io/">Tsung-Wei Ke*<sup>1</sup></a>
                </span>
                &nbsp;
                &nbsp;
                <span class="author-block">
                  <a href="https://sites.google.com/view/sangwoomo">Sangwoo Mo*<sup>2</sup></a>
                </span>
                &nbsp;
                &nbsp;
                <span class="author-block">
                    <a href="https://web.eecs.umich.edu/~stellayu/">Stella Yu<sup>1,2</sup></a>
                </span>
                
              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup>1</sup>University of California Berkeley</span><br>
                <span class="author-block"><sup>2</sup>University of Michigan</span>
              </div>
              
              <div class="is-size-5 contribution">
                <span class="appear">ICLR 2024 (spotlight)</a></span>
              </div>
    
              <div class="is-size-5 contribution">
                <span class="contribution"><sup>*</sup>Equal Contribution</span>
              </div><br> 
    
              <!-- <table align=center width=700px>
                <tr>
                  <td align=center width=100px><center><span style="font-size:28px"><a href="https://arxiv.org/abs/2304.14391">[Paper]</a></span></center></td>
                  <td align=center width=100px><center><span style="font-size:28px"><a href="https://github.com/nickgkan/butd_detr">[Code]</a></span></center></td>
              <tr/>
            </table> -->
    
            <div class="linkscontainer">
              <div class="links mt-4">
              <a href="" target="_blank"><span class="material-symbols-rounded">
              description
              </span><span>Paper</span></a>
              <a href="https://github.com/twke18/CAST" target="_blank"><span class="material-symbols-rounded">
              code
              </span><span>Code</span></a>
              <a href="https://huggingface.co/twke/CAST/tree/main" target="_blank"><span class="material-symbols-rounded">
              science
              </span><span>Checkpoints</span></a>
              <a href="https://openreview.net/forum?id=IRcv4yFX6z" target="_blank"><span class="material-symbols-rounded">
              folder
              </span><span>Review</span></a>
              <!-- <a href="#results"><span class="material-symbols-rounded">
              science
              </span><span>Results</span></a> -->
              </div>
              </div>
    
            </div>
          </div>
        </div>
      </div>
    </section>


    <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">CAST</h2>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column">
                <img src="./static/images/teaser.jpg" alt="input image" style="vertical-align:middle;margin:0px 0px" width="100%"/>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                    Large vision and language models learned directly through image-text associations often lack detailed visual substantiation, whereas image segmentation tasks are treated separately from recognition, supervisedly learned without interconnections. Our key observation is that, while an image can be recognized in multiple ways, each has a consistent part-and-whole visual organization. Segmentation thus should be treated not as an end task to be mastered through supervised learning, but as an internal process that evolves with and supports the ultimate goal of recognition. We propose to integrate a hierarchical segmenter into the recognition process, train and adapt the entire model solely on image-level recognition objectives. We learn hierarchical segmentation for free alongside recognition, automatically uncovering part-to-whole relationships that not only underpin but also enhance recognition. Enhancing the Vision Transformer (ViT) with adaptive segment tokens and graph pooling, our model surpasses ViT in unsupervised part-whole discovery, semantic segmentation, image classification, and efficiency. Notably, our model (trained on unlabeled 1M ImageNet images) outperforms SAM (trained on 11M images and 1 billion masks) by absolute 8% in mIoU on PartImageNet object segmentation.
                    </p>
                </div>
            </div>
          </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Model architecture</h2>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column">
                <img src="./static/images/framework.jpg" alt="input image" style="vertical-align:middle;margin:0px 0px" width="100%"/>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column">
                <div class="content has-text-justified">
                    <p>
                    Our model implements our concept of concurrency and consistency in visual parsing by innovating ViT with adaptive segment tokens and progressive graph pooling. It starts with superpixels instead of square patches, and applies graph pooling to merge fine segments \(\mathbf S_{l-1} \) into coarse segments  \(\mathbf S_{l} \). Both segment transition probability \(\mathbf P_{l} \) and segment feature \(\mathbf Z_{l} \) are learned to optimize an image-level recognition objective, <font color="Blue">which could be self-supervised instance discrimination</font> or <font color="Red">supervised image classification</font>. Without any external supervision, we uncover object wholes (<font color="Red">dog</font>) along with small details (<font color="Brown">ears</font>) and thin structures (<font color="Cyan">legs</font>), validating the effectiveness of our concept.
                    </p>
                </div>
            </div>
          </div>
        </div>
    </section>

    <!-- hack to pull the below up vertically -->
    <span style="display:block; margin-top:-1.75em;"/>

    <section class="section">
        <!-- Zero Shot. -->
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column">
                    <h2 class="title is-3">Discovery of parts from the whole on ImageNet</h2>
                    <div class="content has-text-justified">
                        <p>
                        Without the need of any part annotations, our model generates high-quality hierarchical segmentation on ImageNet and COCO using only an image recognition objective.  Our color scheme has coarse-to-fine consistency: Colors in 8-way segmentations are matched between ViT and CAST, while colors in <font color="Cyan">16</font>(<font color="Red">32</font>)-way segmentations have the same hues as 8-way but vary in <font color="Cyan">saturation</font>(<font color="Red">value</font>) to reflect finer details. Our results more closely follow visual contours and successfully uncover entire objects with details. Our results more closely follow visual contours and successfully uncover entire objects with details like <font color="Red">neck</font>, <font color="Cyan">thin legs</font>, and <font color="Blue">long ears</font>.
                        </p>
                    </div>
                    <div class="content has-text-centered">
                        <img src="./static/images/hierarchical_imagenet.png" alt="input image" style="vertical-align:middle;margin:0px 0px" width="100%"/>
                    </div>
               </div>
            </div>
        </div>
    </section>

    <section class="section">
        <!-- Zero Shot. -->
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column">
                    <h2 class="title is-3">SOTA unsupervised hierarchical segmentation on DensePose</h2>
                    <div class="content has-text-justified">
                        <p>
                        Training CAST and HSG on unlabeled COCO data, we evaluate 32,16,8-way segmentations on ground-truth 14,4,1-label human body parsing respectively on DensePose. We measure precision (P), recall (R), and F-score (F) on between segmentation and ground-truth human body. CAST consistently excels, especially in whole body recall.
                        </p>
                    </div>
                    <div class="table-container is-max-desktop">
                        <table style="width:80%">
                            <tr>
                                <th>P&nbsp;&nbsp; R&nbsp;&nbsp; F</th>
                                <th>14 labels</th>
                                <th>4 labels</th>
                                <th>1 label</th>
                            </tr>
                            <tr>
                                <td colspan="11" style="border-bottom: 1px solid #ddd;"></td>
                            </tr>
                            <tr>
                                <td>HSG</td>
                                <td>20.7&nbsp;&nbsp;18.6&nbsp;&nbsp;19.6</td>
                                <td>24.1&nbsp;&nbsp; 30.6&nbsp;&nbsp; 26.9</td>
                                <td>20.5&nbsp;&nbsp; 36.1&nbsp;&nbsp; 26.2</td>
                            </tr>
                            <tr>
                                <td colspan="11" style="border-bottom: 1px solid #ddd;"></td>
                            </tr>
                            <tr>
                                <td>CAST</td>
                                <td><strong>21.1</strong>&nbsp;&nbsp;<strong>24.1</strong>&nbsp;&nbsp;<strong>22.5</strong></td>
                                <td><strong>24.8</strong>&nbsp;&nbsp;<strong>33.2</strong>&nbsp;&nbsp;<strong>28.4</strong></td>
                                <td><strong>26.3</strong>&nbsp;&nbsp;<strong>44.9</strong>&nbsp;&nbsp;<strong>33.2</strong></td>
                            </tr>

                        </table>
                    </div>
                    <div class="content has-text-centered">
                        <img src="./static/images/hierarchical_densepose.png" alt="input image" style="vertical-align:middle;margin:0px 0px" width="100%"/>
                    </div>
               </div>
            </div>
        </div>
    </section>

    <section class="section">
        <!-- Zero Shot. -->
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column">
                    <h2 class="title is-3">SOTA unsupervised hierarchical segmentation on PartImageNet</h2>
                    <div class="content has-text-justified">
                        <p>
                        Training CAST on unlabeled ImageNet data, we classify 16,8-way segmentations into object and part categories with an open vocabulary classifier (OVSeg). We measure region mIoU and boundary F-score, at first object and then part levels. CAST outperforms SAM by a large margin.
                        </p>
                    </div>
                    <div class="table-container is-max-desktop">
                        <table style="width:80%">
                            <tr>
                                <th></th>
                                <th></th>
                                <th colspan="2">region mIoU</th>
                                <th colspan="2">boundary F-score</th>
                            </tr>
                            <tr>
                                <th>model</th>
                                <th>GFLOPS</th>
                                <th>object</th>
                                <th>part</th>
                                <th>object</th>
                                <th>part</th>
                            </tr>

                            <tr>
                                <td colspan="11" style="border-bottom: 1px solid #ddd;"></td>
                            </tr>
                            <tr>
                                <td>SAM-B</td>
                                <td>677</td>
                                <td>18.03</td>
                                <td>10.15</td>
                                <td>20.71</td>
                                <td>7.25</td>
                            </tr>
                            <tr>
                                <td>SAM-H</td>
                                <td>3166</td>
                                <td>21.97</td>
                                <td>12.07</td>
                                <td><strong>32.66</strong></td>
                                <td><strong>11.82</strong></td>
                            </tr>
                            <tr>
                                <td colspan="11" style="border-bottom: 1px solid #ddd;"></td>
                            </tr>
                            <tr>
                                <td>ViT</td>
                                <td>18</td>
                                <td>25.34</td>
                                <td>11.74</td>
                                <td>10.92</td>
                                <td>4.64</td>
                            </tr>
                            <tr>
                                <td>CAST</td>
                                <td><strong>18</strong></td>
                                <td><strong>29.66</strong></td>
                                <td><strong>13.20</strong></td>
                                <td>22.32</td>
                                <td>6.52</td>
                            </tr>
                        </table>
                    </div>
                    <div class="content has-text-centered">
                        <img src="./static/images/partimagenet.jpg" alt="input image" style="vertical-align:middle;margin:0px 0px" width="100%"/>
                    </div>
               </div>
            </div>
        </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <pre><code>@inproceedings{ke2023cast,
            title={Learning Hierarchical Image Segmentation For Recognition and By Recognition},
            author={Ke, Tsung-Wei and Mo, Sangwoo and Stella, X Yu},
            booktitle={The Twelfth International Conference on Learning Representations},
            year={2023}
}</code></pre>
        </div>
      </section>

    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <!-- TODO: UPDATE -->
                <a class="icon-link" href="" target="_blank">
                    <i class="ai ai-arxiv"></i>
                </a>
                &nbsp;
                <!-- TODO: UPDATE -->
                <a class="icon-link" href="" target="_blank">
                    <i class="fas fa-file-pdf"></i>
                </a>
                &nbsp;
                <a class="icon-link" href="https://github.com/twke18/CAST"
                    target="_blank">
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="content">
                    <p>
                        Page source code was adapted from
                        <a href="https://nerfies.github.io" target="_blank">here</a>
                        and
                        <a href="https://ebmplanner.github.io/"
                            target="_blank">here</a>,
                        <a href="https://github.com/3d-diffuser-actor/3d-diffuser-actor.github.io"
                            target="_blank">here</a>,
                        and can be found in <a
                            href="https://github.com/CAST-vision/CAST.github.io"
                            target="_blank">this repository</a>.
                    </p>
                </div>
            </div>
    </footer>

    <!-- <script src="./static/js/index.js"></script>
    <script src="./static/js/prism.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.0.1/prism-bibtex.js"
        integrity="sha256-+dK6uqUp/DnP6ef97s8XcoynBnGe5vM5gvBECH0EB3U=" crossorigin="anonymous">
        </script> -->
</body>

</html>
